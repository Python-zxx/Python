机器学习基本步骤：
    特征工程，确定模型需要学习的特征————添加训练样本————机器训练，提取特征————总结特征，得到分类模型————用测试数据集验证模型
                                 (包含数据信息和对应结果)

聚类：（无监督学习）将数据进行分类的过程。
    利用向量之间的距离，根据距离的大小来判断对象是否应该归为同一类别。
    
回归：由果索因的过程。

线性回归：y=f（x）=wx+b
    其中w是一特征张量，储存着与每个变量x中元素对应的特征元素，x就是输入的训练数据张量，b是一个偏量值。
    即根据一堆（x，y）求解一个合适的w和b。
    
损失函数 LOSS 用来评估模型预测结果，越小说明模型越好。
    累加 |wx+b-y|
    
梯度下降法： 通过迭代，逐步逼近所要寻找的极值，且人为引入一个η参数，用于调整步长。
普通梯度下降法（批度下降法BGD）：遍历所有数据样本
随机梯度下降法（SGD）：每次随机抽取样本总体中的一个样本，来决定下一步的走向。（可加快收敛速度，会降低精度）
小批梯度下降法（Mini-batch GD）：每次选取一定量的样本进行训练，然后再更新权值。

    对于一系列的事件而言，根据其历史发生数据可以计算出一个先验概率，再对这个概率进行对数运算后可得到该事件发生的信息量。
信息量：通过对一种情况发生的先验概率进行对数计算的结果。
信息熵：将信息量乘以先验概率，可得到单个事件的熵。再将这些单个事件的熵求和，就可得到信息熵。
信息熵的作用：量化信息量。如果越不确定，则信息熵就越大。

交叉熵：用来表示训练结果和实际标签结果的差距。
